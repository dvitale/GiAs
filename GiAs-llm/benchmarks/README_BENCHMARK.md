# GiAs-llm Backend Comparison Tool

Tool completo per confrontare le performance tra **Ollama** e **Llama.cpp** come backend LLM per GiAs-llm.

## üìã Indice

- [Caratteristiche](#caratteristiche)
- [Prerequisiti](#prerequisiti)
- [Installazione](#installazione)
- [Utilizzo](#utilizzo)
- [Test Cases](#test-cases)
- [Output](#output)
- [Esempi](#esempi)
- [Interpretazione Risultati](#interpretazione-risultati)

---

## ‚ú® Caratteristiche

- ‚úÖ **42+ test cases** coprendo tutti gli intent principali
- ‚úÖ **Metriche multiple**: Accuratezza, tempo di risposta, stabilit√†
- ‚úÖ **Report dettagliati**: Console, JSON, HTML interattivo
- ‚úÖ **Statistiche aggregate**: Min, Max, Media, Deviazione Standard
- ‚úÖ **Analisi per intent**: Performance granulare
- ‚úÖ **Quick mode**: Test rapidi per verifiche veloci
- ‚úÖ **Grafici interattivi**: Visualizzazione chiara dei risultati
- ‚úÖ **Iterazioni multiple**: Risultati affidabili e riproducibili

---

## üì¶ Prerequisiti

### 1. Backend LLM Attivi

**Ollama** (porta 11434):
```bash
# Verifica che Ollama sia in esecuzione
curl -s http://localhost:11434/api/tags

# Se non √® attivo, avvialo
ollama serve
```

**Llama.cpp** (porta 11435):
```bash
# Verifica che llama.cpp sia in esecuzione
curl -s http://localhost:11435/health

# Se non √® attivo, avvialo
cd /opt/lang-env/GiAs-llm
./start_llama-cpp.sh
```

### 2. Dipendenze Python

Il tool usa solo librerie standard Python 3:
- `json`, `time`, `statistics`, `argparse`, `dataclasses`

Nessuna installazione aggiuntiva richiesta!

---

## üöÄ Utilizzo

### Benchmark Completo (Consigliato)

Esegue tutti i 42 test cases con 3 iterazioni per backend:

```bash
cd /opt/lang-env/GiAs-llm/benchmarks
python3 compare_llm_backends.py
```

**Output:**
- Report dettagliato su console
- File JSON: `benchmark_results.json`

**Tempo stimato:** 5-10 minuti

---

### Quick Test (Veloce)

Esegue 10 test cases rappresentativi con 1 iterazione:

```bash
python3 compare_llm_backends.py --quick
```

**Output:**
- Report su console
- File JSON: `benchmark_results.json`

**Tempo stimato:** 1-2 minuti

Perfetto per verifiche rapide dopo modifiche al sistema!

---

### Test Singolo Backend

Testa solo Llama.cpp:

```bash
python3 compare_llm_backends.py --backends llamacpp
```

Testa solo Ollama:

```bash
python3 compare_llm_backends.py --backends ollama
```

---

### Configurazione Avanzata

```bash
python3 compare_llm_backends.py \
  --backends ollama llamacpp \
  --iterations 5 \
  --output my_benchmark.json \
  --quiet
```

**Opzioni:**
- `--backends`: Backend da testare (default: entrambi)
- `--iterations`: Numero di iterazioni per test (default: 3)
- `--output`: Nome file JSON output (default: benchmark_results.json)
- `--quick`: Quick mode (10 test, 1 iterazione)
- `--quiet`: Output minimale (solo report finale)

---

## üìä Visualizzazione Risultati

### Report HTML Interattivo

Genera report HTML con grafici Chart.js:

```bash
python3 visualize_benchmark.py benchmark_results.json
```

**Output:**
- `benchmark_report.html` - Report interattivo con grafici

Apri il file nel browser per visualizzare:
- üìä Grafici accuratezza e tempi
- üìã Tabella statistiche dettagliate
- üèÜ Indicatori vincitore per metrica
- üìà Analisi comparativa

### Summary Testuale

```bash
python3 visualize_benchmark.py benchmark_results.json --summary
```

Stampa solo il riepilogo testuale senza generare HTML.

---

## üß™ Test Cases

Il benchmark include **42 test cases** organizzati in categorie:

### Categorie

| Categoria | Test Cases | Complessit√† |
|-----------|------------|-------------|
| **Saluti e Aiuto** | 8 | Simple |
| **Piani - Descrizione** | 3 | Medium |
| **Piani - Stabilimenti** | 3 | Medium-Complex |
| **Piani - Generic** | 3 | Medium |
| **Piani - Statistiche** | 3 | Medium |
| **Ricerca Piani** | 3 | Medium-Complex |
| **Priorit√† Controlli** | 3 | Medium |
| **Analisi Rischio** | 6 | Medium-Complex |
| **Mai Controllati** | 3 | Medium |
| **Piani in Ritardo** | 4 | Medium-Complex |
| **Storico Stabilimenti** | 2 | Complex |
| **Non Conformit√†** | 2 | Complex |

### Intent Testati

```
‚úÖ greet, goodbye, ask_help
‚úÖ ask_piano_description
‚úÖ ask_piano_stabilimenti
‚úÖ ask_piano_generic
‚úÖ ask_piano_statistics
‚úÖ search_piani_by_topic
‚úÖ ask_priority_establishment
‚úÖ ask_risk_based_priority
‚úÖ ask_top_risk_activities
‚úÖ ask_suggest_controls
‚úÖ ask_delayed_plans
‚úÖ check_if_plan_delayed
‚úÖ ask_establishment_history
‚úÖ analyze_nc_by_category
```

---

## üìÑ Output

### 1. Report Console

```
================================================================================
                         BENCHMARK COMPARISON REPORT
================================================================================

üìä OVERALL STATISTICS
--------------------------------------------------------------------------------
Backend         Tests    Correct    Accuracy     Avg Time (ms)   Std Dev
--------------------------------------------------------------------------------
LLAMACPP        126      120        95.24%            842.35        127.45
OLLAMA          126      118        93.65%           1156.78        189.32
--------------------------------------------------------------------------------

üîç DIRECT COMPARISON: LLAMACPP vs OLLAMA
--------------------------------------------------------------------------------
Accuracy:        LLAMACPP wins by 1.59%
Speed:           LLAMACPP is 314.43ms faster (27.2% improvement)

üìã ACCURACY BY INTENT
--------------------------------------------------------------------------------

greet:
  LLAMACPP        Accuracy:  100.00% (9/9)  Avg Time:  245.67ms
  OLLAMA          Accuracy:  100.00% (9/9)  Avg Time:  387.23ms

ask_piano_description:
  LLAMACPP        Accuracy:  100.00% (9/9)  Avg Time:  892.45ms
  OLLAMA          Accuracy:   88.89% (8/9)  Avg Time: 1234.12ms

...
```

### 2. File JSON

```json
{
  "timestamp": "2025-01-28T16:45:23.123456",
  "test_config": {
    "test_cases": 42,
    "iterations": 3,
    "backends": ["ollama", "llamacpp"]
  },
  "statistics": {
    "llamacpp": {
      "backend": "llamacpp",
      "total_tests": 126,
      "correct": 120,
      "accuracy": 95.24,
      "avg_response_time_ms": 842.35,
      ...
    }
  },
  "detailed_results": {
    "llamacpp": [
      {
        "backend": "llamacpp",
        "intent": "greet",
        "message": "ciao",
        "expected_intent": "greet",
        "predicted_intent": "greet",
        "correct": true,
        "response_time_ms": 245.67,
        ...
      }
    ]
  }
}
```

### 3. Report HTML

Report interattivo con:
- üìä **Grafici a barre** per accuratezza e tempi
- üìã **Tabella dettagliata** con tutte le statistiche
- üèÜ **Badge vincitore** per ogni metrica
- üé® **Design responsive** e professionale
- üì± **Mobile-friendly**

---

## üìà Interpretazione Risultati

### Metriche Chiave

#### 1. **Accuracy (%)**
- Percentuale di intent classificati correttamente
- **Target:** ‚â• 95% per production
- **Buono:** 90-95%
- **Accettabile:** 85-90%
- **Problema:** < 85%

#### 2. **Avg Response Time (ms)**
- Tempo medio di classificazione
- **Eccellente:** < 500ms
- **Buono:** 500-1000ms
- **Accettabile:** 1000-2000ms
- **Lento:** > 2000ms

#### 3. **Std Dev (ms)**
- Stabilit√† delle performance
- **Stabile:** < 100ms
- **Medio:** 100-200ms
- **Instabile:** > 200ms

#### 4. **Min/Max Time (ms)**
- Range di variazione
- Indica outlier e worst-case

### Quando Preferire Llama.cpp

‚úÖ **Velocit√† critica** (< 1s risposta)
‚úÖ **Risorse limitate** (memoria/CPU)
‚úÖ **Alto throughput** (molte richieste/sec)
‚úÖ **Costo operativo** (minore consumo risorse)

### Quando Preferire Ollama

‚úÖ **Accuratezza massima** (> 95%)
‚úÖ **Modelli diversi** (facile switching)
‚úÖ **Debugging** (migliori log e tools)
‚úÖ **Ecosistema** (community e supporto)

---

## üîß Troubleshooting

### Errore: Backend non disponibile

```
‚ö†Ô∏è Warning: llamacpp not available (connection refused)
```

**Soluzione:**
```bash
# Verifica che il server sia attivo
curl http://localhost:11435/health  # Llama.cpp
curl http://localhost:11434/api/tags  # Ollama

# Se non risponde, avvia il server
./start_llama-cpp.sh  # per Llama.cpp
ollama serve           # per Ollama
```

### Errore: Import config

```
ImportError: No module named 'configs'
```

**Soluzione:**
```bash
# Esegui dalla directory corretta
cd /opt/lang-env/GiAs-llm/benchmarks
python3 compare_llm_backends.py
```

### Performance Inattese

Se i risultati sono molto diversi dal previsto:

1. **Warmup insufficiente**: Il primo test √® sempre pi√π lento
   - Soluzione: Usa `--iterations 3` o pi√π

2. **Cache attiva**: I test successivi sono pi√π veloci
   - Il tool disabilita la cache automaticamente

3. **Sistema carico**: Altri processi rallentano i test
   - Soluzione: Esegui su sistema dedicato o con carico basso

4. **Modello diverso**: Ollama potrebbe usare un modello diverso
   - Soluzione: Verifica configurazione in `config.json`

---

## üìö Esempi Pratici

### Esempio 1: Test Rapido Prima di Deploy

```bash
# Quick test per verificare che tutto funzioni
python3 compare_llm_backends.py --quick --quiet

# Se OK, procedi con test completo
python3 compare_llm_backends.py --output pre-deploy.json
```

### Esempio 2: Ottimizzazione Backend

```bash
# Test solo Llama.cpp con molte iterazioni
python3 compare_llm_backends.py \
  --backends llamacpp \
  --iterations 10 \
  --output llamacpp-optimized.json

# Analizza risultati
python3 visualize_benchmark.py llamacpp-optimized.json
```

### Esempio 3: Confronto Dopo Aggiornamento

```bash
# Test PRIMA dell'aggiornamento
python3 compare_llm_backends.py --output before.json

# ... aggiorna il sistema ...

# Test DOPO l'aggiornamento
python3 compare_llm_backends.py --output after.json

# Confronta i due file JSON manualmente
```

### Esempio 4: Benchmark per Documentazione

```bash
# Test completo con 5 iterazioni per risultati affidabili
python3 compare_llm_backends.py \
  --iterations 5 \
  --output official-benchmark.json

# Genera report HTML professionale
python3 visualize_benchmark.py official-benchmark.json \
  --output official-report.html

# Condividi official-report.html nella documentazione
```

---

## üéØ Best Practices

1. **Warmup**: I primi test sono sempre pi√π lenti
   - Usa almeno 3 iterazioni

2. **Sistema Pulito**: Esegui su sistema non carico
   - Chiudi applicazioni pesanti
   - Evita altri processi LLM

3. **Ripetibilit√†**: Usa lo stesso numero di iterazioni
   - Default 3 √® un buon compromesso

4. **Documentazione**: Salva i risultati con nomi descrittivi
   - `benchmark-v1.2-llamacpp.json`
   - `production-test-2025-01-28.json`

5. **Monitoraggio**: Esegui benchmark periodici
   - Dopo ogni aggiornamento major
   - Mensilmente per tracciare trend

---

## üìû Supporto

Per problemi o domande:
- Verifica questo README
- Controlla i log dei server LLM
- Testa i backend individualmente
- Controlla la configurazione in `config.json`

---

## üîÑ Aggiornamenti

**Versione:** 1.0.0
**Data:** 2025-01-28
**Autore:** GiAs-llm Development Team

---

## üìù Licenza

Parte del progetto GiAs-llm - Regione Campania
